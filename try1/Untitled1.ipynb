{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91bd6ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:31:44.427808Z",
     "start_time": "2023-07-18T08:31:43.306745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rh987\\AppData\\Local\\Temp\\ipykernel_26008\\2642917225.py:7: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from functools import partial\n",
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f357e719",
   "metadata": {},
   "source": [
    "# data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7624281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:35:14.357874Z",
     "start_time": "2023-07-18T08:35:14.352554Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 416\n",
    "BATCH_SIZE = 4\n",
    "scale = 1.1\n",
    "\n",
    "yolo_max_boxes= 100 # maximum number of detections at one time')\n",
    "yolo_iou_threshold=0.5 # iou threshold\n",
    "yolo_score_threshold= 0.5 # score threshold\n",
    "\n",
    "yolo_anchors = np.array([(10, 13), (16, 30), (33, 23), (30, 61), (62, 45),\n",
    "                         (59, 119), (116, 90), (156, 198), (373, 326)],\n",
    "                        np.float32) / 416\n",
    "yolo_anchor_masks = np.array([[6, 7, 8], [3, 4, 5], [0, 1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f11a3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:31:56.428054Z",
     "start_time": "2023-07-18T08:31:56.401828Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rh987\\AppData\\Roaming\\Python\\Python39\\site-packages\\albumentations\\imgaug\\transforms.py:346: FutureWarning: This IAAAffine is deprecated. Please use Affine instead\n",
      "  warnings.warn(\"This IAAAffine is deprecated. Please use Affine instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=int(IMAGE_SIZE * scale),\n",
    "            min_width=int(IMAGE_SIZE * scale),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "        ),\n",
    "        A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),\n",
    "        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ShiftScaleRotate(\n",
    "                    rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "                A.IAAAffine(shear=15, p=0.5, mode=\"constant\"),\n",
    "            ],\n",
    "            p=1.0,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Blur(p=0.1),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.Posterize(p=0.1),\n",
    "        A.ToGray(p=0.1),\n",
    "        A.ChannelShuffle(p=0.05),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"pascal_voc\", min_visibility=0.4),\n",
    "\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=IMAGE_SIZE),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"pascal_voc\", min_visibility=0.4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9713e825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:31:56.920714Z",
     "start_time": "2023-07-18T08:31:56.906585Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def transform_targets_for_output(y_true, grid_size, anchor_idxs):\n",
    "    # y_true: (N, boxes, (x1, y1, x2, y2, class, best_anchor))\n",
    "    N = tf.shape(y_true)[0]\n",
    "\n",
    "    # y_true_out: (N, grid, grid, anchors, [x, y, w, h, obj, class])\n",
    "    y_true_out = tf.zeros((N, grid_size, grid_size, tf.shape(anchor_idxs)[0], 6))\n",
    "\n",
    "    anchor_idxs = tf.cast(anchor_idxs, tf.int32)\n",
    "\n",
    "    indexes = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "    idx = 0\n",
    "    for i in tf.range(N):\n",
    "        for j in tf.range(tf.shape(y_true)[1]):\n",
    "            if tf.equal(y_true[i][j][2], 0):\n",
    "                continue\n",
    "            anchor_eq = tf.equal(\n",
    "                anchor_idxs, tf.cast(y_true[i][j][5], tf.int32))\n",
    "\n",
    "            if tf.reduce_any(anchor_eq):\n",
    "                box = y_true[i][j][0:4]\n",
    "                box_xy = (y_true[i][j][0:2] + y_true[i][j][2:4]) / 2\n",
    "\n",
    "                anchor_idx = tf.cast(tf.where(anchor_eq), tf.int32)\n",
    "                grid_xy = tf.cast(box_xy // (1/grid_size), tf.int32)\n",
    "\n",
    "                # grid[y][x][anchor] = (tx, ty, bw, bh, obj, class)\n",
    "                indexes = indexes.write(idx, [i, grid_xy[1], grid_xy[0], anchor_idx[0][0]])\n",
    "                updates = updates.write(idx, [box[0], box[1], box[2], box[3], 1, y_true[i][j][4]])\n",
    "                idx += 1\n",
    "\n",
    "    return tf.tensor_scatter_nd_update(y_true_out, indexes.stack(), updates.stack())\n",
    "\n",
    "def _transform_targets(anchors, anchor_masks, size):\n",
    "    def transform_targets(y_train):\n",
    "        y_outs = []\n",
    "        grid_size = size // 32\n",
    "\n",
    "        # calculate anchor index for true boxes\n",
    "        anchors = tf.cast(anchors, tf.float32)\n",
    "        anchor_area = anchors[..., 0] * anchors[..., 1]\n",
    "        box_wh = y_train[..., 2:4] - y_train[..., 0:2]\n",
    "        box_wh = tf.tile(tf.expand_dims(box_wh, -2),(1, 1, tf.shape(anchors)[0], 1))\n",
    "        box_area = box_wh[..., 0] * box_wh[..., 1]\n",
    "        intersection = tf.minimum(box_wh[..., 0], anchors[..., 0]) * tf.minimum(box_wh[..., 1], anchors[..., 1])\n",
    "        iou = intersection / (box_area + anchor_area - intersection)\n",
    "        anchor_idx = tf.cast(tf.argmax(iou, axis=-1), tf.float32)\n",
    "        anchor_idx = tf.expand_dims(anchor_idx, axis=-1)\n",
    "\n",
    "        y_train = tf.concat([y_train, anchor_idx], axis=-1)\n",
    "\n",
    "        for anchor_idxs in anchor_masks:\n",
    "            y_outs.append(transform_targets_for_output(y_train, grid_size, anchor_idxs))\n",
    "            grid_size *= 2\n",
    "        return tuple(y_outs)\n",
    "    return transform_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e012b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:31:57.497785Z",
     "start_time": "2023-07-18T08:31:57.482439Z"
    }
   },
   "outputs": [],
   "source": [
    "def aug_fn(image, bbox, train):\n",
    "    data = {\"image\":image,\"bboxes\":bbox}\n",
    "    if train:\n",
    "        aug_data = train_transforms(**data)\n",
    "    else:\n",
    "        aug_data = test_transforms(**data)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    aug_bbox = aug_data[\"bboxes\"]\n",
    "    aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "    aug_bbox = tf.cast(aug_bbox, tf.float32)\n",
    "    return aug_img, aug_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a39df02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:31:58.317237Z",
     "start_time": "2023-07-18T08:31:58.297191Z"
    }
   },
   "outputs": [],
   "source": [
    "def _process_aug(train):\n",
    "    def process_aug(image, bbox):\n",
    "        N = bbox.shape[0]\n",
    "        aug_img, aug_bbox = tf.numpy_function(func=aug_fn, inp=[image, bbox, train],Tout=[tf.float32, tf.float32])\n",
    "        aug_img.set_shape( (416,416,3) )\n",
    "        aug_bbox.set_shape( (N,4) )\n",
    "        return aug_img, aug_bbox\n",
    "    return process_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04f244ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:39:14.642849Z",
     "start_time": "2023-07-18T08:39:14.631491Z"
    }
   },
   "outputs": [],
   "source": [
    "def _parse_tfrecord(train):\n",
    "\n",
    "    def parse_tfrecord(tfrecord):\n",
    "        features = {\n",
    "            'filename': tf.io.FixedLenFeature([], tf.string),\n",
    "#             'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "#             'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'classes': tf.io.VarLenFeature(tf.int64),\n",
    "#             'objectness': tf.io.VarLenFeature(tf.int64),\n",
    "#             'target_id': tf.io.VarLenFeature(tf.int64),\n",
    "            'x_min': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_min': tf.io.VarLenFeature(tf.float32),\n",
    "            'x_max': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_max': tf.io.VarLenFeature(tf.float32),\n",
    "            'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "\n",
    "        parsed_example = tf.io.parse_single_example(tfrecord, features)\n",
    "        img = tf.image.decode_jpeg(parsed_example['image_raw'], channels=3)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        \n",
    "#         width = tf.cast(parsed_example['width'], tf.float32)\n",
    "#         height = tf.cast(parsed_example['height'], tf.float32)\n",
    "        \n",
    "        labels = tf.sparse.to_dense(parsed_example['classes'])\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "#         objectness = tf.sparse.to_dense(parsed_example['objectness'])\n",
    "#         objectness = tf.cast(objectness, tf.float32)\n",
    "#         target_id = tf.sparse.to_dense(parsed_example['target_id'])\n",
    "#         target_id = tf.cast(target_id, tf.float32)\n",
    "        \n",
    "        bboxes = tf.stack(\n",
    "            [\n",
    "                tf.sparse.to_dense(parsed_example['x_min']),\n",
    "                tf.sparse.to_dense(parsed_example['y_min']),\n",
    "                tf.sparse.to_dense(parsed_example['x_max']),\n",
    "                tf.sparse.to_dense(parsed_example['y_max']),\n",
    "            ],axis=1)\n",
    "        \n",
    "        img, bboxes = _process_aug(train)(img, bboxes)\n",
    "        \n",
    "\n",
    "        y_train = tf.concat([bboxes,tf.expand_dims(labels, axis=-1)] , axis=1)\n",
    "        paddings = [[0, yolo_max_boxes - tf.shape(y_train)[0]], [0, 0]]\n",
    "        \n",
    "        y_train = tf.pad(y_train, paddings)\n",
    "        y_train = _transform_targets(yolo_anchors, yolo_anchor_masks, IMAGE_SIZE)(y_train)\n",
    "        \n",
    "        return img,  y_train\n",
    "    \n",
    "    return parse_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32e07163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:42.881763Z",
     "start_time": "2023-07-18T08:52:42.869947Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_tfrecord_dataset(tfrecord_name, train=True, buffer_size=1024):\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n",
    "    raw_dataset = raw_dataset.cache() # 처음 불러오게 되면 캐시메모리로 저장\n",
    "    \n",
    "    if train:\n",
    "        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size) # 데이터 셔플\n",
    "        \n",
    "    # 데이터 읽기 시작\n",
    "    raw_dataset = raw_dataset.map(_parse_tfrecord(train), num_parallel_calls=tf.data.experimental.AUTOTUNE)  \n",
    "\n",
    "    raw_dataset = raw_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    raw_dataset = raw_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62e3a598",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:43.658082Z",
     "start_time": "2023-07-18T08:52:43.628806Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_TFRECORD_PATH = \"./EDA/train_dataset.tfrecord\"\n",
    "\n",
    "train_dataset = load_tfrecord_dataset(\n",
    "    tfrecord_name=TRAIN_TFRECORD_PATH,\n",
    "    train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d2454",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "460fc401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:44.028896Z",
     "start_time": "2023-07-18T08:52:44.012523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Add,\n",
    "    Concatenate,\n",
    "    Conv2D,\n",
    "    Input,\n",
    "    Lambda,\n",
    "    LeakyReLU,\n",
    "    MaxPool2D,\n",
    "    UpSampling2D,\n",
    "    ZeroPadding2D,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import (\n",
    "    binary_crossentropy,\n",
    "    sparse_categorical_crossentropy\n",
    ")\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd93510a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:44.260954Z",
     "start_time": "2023-07-18T08:52:44.245254Z"
    }
   },
   "outputs": [],
   "source": [
    "def broadcast_iou(box_1, box_2):\n",
    "    # box_1: (..., (x1, y1, x2, y2))\n",
    "    # box_2: (N, (x1, y1, x2, y2))\n",
    "\n",
    "    # broadcast boxes\n",
    "    box_1 = tf.expand_dims(box_1, -2)\n",
    "    box_2 = tf.expand_dims(box_2, 0)\n",
    "    # new_shape: (..., N, (x1, y1, x2, y2))\n",
    "    new_shape = tf.broadcast_dynamic_shape(tf.shape(box_1), tf.shape(box_2))\n",
    "    box_1 = tf.broadcast_to(box_1, new_shape)\n",
    "    box_2 = tf.broadcast_to(box_2, new_shape)\n",
    "\n",
    "    int_w = tf.maximum(tf.minimum(box_1[..., 2], box_2[..., 2]) -\n",
    "                       tf.maximum(box_1[..., 0], box_2[..., 0]), 0)\n",
    "    int_h = tf.maximum(tf.minimum(box_1[..., 3], box_2[..., 3]) -\n",
    "                       tf.maximum(box_1[..., 1], box_2[..., 1]), 0)\n",
    "    int_area = int_w * int_h\n",
    "    box_1_area = (box_1[..., 2] - box_1[..., 0]) * \\\n",
    "        (box_1[..., 3] - box_1[..., 1])\n",
    "    box_2_area = (box_2[..., 2] - box_2[..., 0]) * \\\n",
    "        (box_2[..., 3] - box_2[..., 1])\n",
    "    return int_area / (box_1_area + box_2_area - int_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "187fff20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:44.430902Z",
     "start_time": "2023-07-18T08:52:44.420479Z"
    }
   },
   "outputs": [],
   "source": [
    "class BatchNormalization(tf.keras.layers.BatchNormalization):\n",
    "    \"\"\"\n",
    "    Make trainable=False freeze BN for real (the og version is sad)\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if training is None:\n",
    "            training = tf.constant(False)\n",
    "        training = tf.logical_and(training, self.trainable)\n",
    "        return super().call(x, training)\n",
    "    \n",
    "def DarknetConv(x, filters, size, strides=1, batch_norm=True):\n",
    "    if strides == 1:\n",
    "        padding = 'same'\n",
    "    else:\n",
    "        x = ZeroPadding2D(((1, 0), (1, 0)))(x)  # top left half-padding\n",
    "        padding = 'valid'\n",
    "    x = Conv2D(filters=filters, kernel_size=size,\n",
    "               strides=strides, padding=padding,\n",
    "               use_bias=not batch_norm, kernel_regularizer=l2(0.0005))(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def DarknetResidual(x, filters):\n",
    "    prev = x\n",
    "    x = DarknetConv(x, filters // 2, 1)\n",
    "    x = DarknetConv(x, filters, 3)\n",
    "    x = Add()([prev, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def DarknetBlock(x, filters, blocks):\n",
    "    x = DarknetConv(x, filters, 3, strides=2)\n",
    "    for _ in range(blocks):\n",
    "        x = DarknetResidual(x, filters)\n",
    "    return x\n",
    "\n",
    "\n",
    "def Darknet(name=None):\n",
    "    x = inputs = Input([None, None, 3])\n",
    "    x = DarknetConv(x, 32, 3)\n",
    "    x = DarknetBlock(x, 64, 1)\n",
    "    x = DarknetBlock(x, 128, 2)  # skip connection\n",
    "    x = x_36 = DarknetBlock(x, 256, 8)  # skip connection\n",
    "    x = x_61 = DarknetBlock(x, 512, 8)\n",
    "    x = DarknetBlock(x, 1024, 4)\n",
    "    return tf.keras.Model(inputs, (x_36, x_61, x), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3d4c33a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:44.757026Z",
     "start_time": "2023-07-18T08:52:44.741176Z"
    }
   },
   "outputs": [],
   "source": [
    "def YoloConv(filters, name=None):\n",
    "    def yolo_conv(x_in):\n",
    "        if isinstance(x_in, tuple):\n",
    "            inputs = Input(x_in[0].shape[1:]), Input(x_in[1].shape[1:])\n",
    "            x, x_skip = inputs\n",
    "\n",
    "            # concat with skip connection\n",
    "            x = DarknetConv(x, filters, 1)\n",
    "            x = UpSampling2D(2)(x)\n",
    "            x = Concatenate()([x, x_skip])\n",
    "        else:\n",
    "            x = inputs = Input(x_in.shape[1:])\n",
    "\n",
    "        x = DarknetConv(x, filters, 1)\n",
    "        x = DarknetConv(x, filters * 2, 3)\n",
    "        x = DarknetConv(x, filters, 1)\n",
    "        x = DarknetConv(x, filters * 2, 3)\n",
    "        x = DarknetConv(x, filters, 1)\n",
    "        return Model(inputs, x, name=name)(x_in)\n",
    "    return yolo_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0883af5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:45.002546Z",
     "start_time": "2023-07-18T08:52:44.996414Z"
    }
   },
   "outputs": [],
   "source": [
    "def YoloOutput(filters, anchors, classes, name=None):\n",
    "    def yolo_output(x_in):\n",
    "        x = inputs = Input(x_in.shape[1:])\n",
    "        x = DarknetConv(x, filters * 2, 3)\n",
    "        x = DarknetConv(x, anchors * (classes + 5), 1, batch_norm=False)\n",
    "        x = Lambda(lambda x: tf.reshape(x, (-1, tf.shape(x)[1], tf.shape(x)[2],\n",
    "                                            anchors, classes + 5)))(x)\n",
    "        return tf.keras.Model(inputs, x, name=name)(x_in)\n",
    "    return yolo_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e6c443f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:45.204980Z",
     "start_time": "2023-07-18T08:52:45.190648Z"
    }
   },
   "outputs": [],
   "source": [
    "def yolo_boxes(pred, anchors, classes):\n",
    "    # pred: (batch_size, grid, grid, anchors, (x, y, w, h, obj, ...classes))\n",
    "    grid_size = tf.shape(pred)[1]\n",
    "    box_xy, box_wh, objectness, class_probs = tf.split(pred, (2, 2, 1, classes), axis=-1)\n",
    "\n",
    "    box_xy = tf.sigmoid(box_xy)\n",
    "    objectness = tf.sigmoid(objectness)\n",
    "    class_probs = tf.sigmoid(class_probs)\n",
    "    pred_box = tf.concat((box_xy, box_wh), axis=-1)  # original xywh for loss\n",
    "\n",
    "    # !!! grid[x][y] == (y, x)\n",
    "    grid = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n",
    "    grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)  # [gx, gy, 1, 2]\n",
    "\n",
    "    box_xy = (box_xy + tf.cast(grid, tf.float32)) / tf.cast(grid_size, tf.float32)\n",
    "    box_wh = tf.exp(box_wh) * anchors\n",
    "\n",
    "    box_x1y1 = box_xy - box_wh / 2\n",
    "    box_x2y2 = box_xy + box_wh / 2\n",
    "    bbox = tf.concat([box_x1y1, box_x2y2], axis=-1)\n",
    "\n",
    "    return bbox, objectness, class_probs, pred_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4562fca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:45.402863Z",
     "start_time": "2023-07-18T08:52:45.381005Z"
    }
   },
   "outputs": [],
   "source": [
    "def yolo_nms(outputs, anchors, masks, classes):\n",
    "    # boxes, conf, type\n",
    "    b, c, t = [], [], []\n",
    "\n",
    "    for o in outputs:\n",
    "        b.append(tf.reshape(o[0], (tf.shape(o[0])[0], -1, tf.shape(o[0])[-1])))\n",
    "        c.append(tf.reshape(o[1], (tf.shape(o[1])[0], -1, tf.shape(o[1])[-1])))\n",
    "        t.append(tf.reshape(o[2], (tf.shape(o[2])[0], -1, tf.shape(o[2])[-1])))\n",
    "\n",
    "    bbox = tf.concat(b, axis=1)\n",
    "    confidence = tf.concat(c, axis=1)\n",
    "    class_probs = tf.concat(t, axis=1)\n",
    "\n",
    "    scores = confidence * class_probs\n",
    "    boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(\n",
    "        boxes=tf.reshape(bbox, (tf.shape(bbox)[0], -1, 1, 4)),\n",
    "        scores=tf.reshape(scores, (tf.shape(scores)[0], -1, tf.shape(scores)[-1])),\n",
    "        max_output_size_per_class=FLAGS.yolo_max_boxes,\n",
    "        max_total_size=FLAGS.yolo_max_boxes,\n",
    "        iou_threshold=FLAGS.yolo_iou_threshold,\n",
    "        score_threshold=FLAGS.yolo_score_threshold\n",
    "    )\n",
    "\n",
    "    return boxes, scores, classes, valid_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23ef4a46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:45.556446Z",
     "start_time": "2023-07-18T08:52:45.532747Z"
    }
   },
   "outputs": [],
   "source": [
    "def YoloV3(size=None,\n",
    "           channels=3,\n",
    "           anchors=yolo_anchors,\n",
    "           masks=yolo_anchor_masks,\n",
    "           classes=80,\n",
    "           training=False):\n",
    "    \n",
    "    x = inputs = Input([size, size, channels], name='input')\n",
    "\n",
    "    x_36, x_61, x = Darknet(name='yolo_darknet')(x)\n",
    "\n",
    "    x = YoloConv(512, name='yolo_conv_0')(x)\n",
    "    output_0 = YoloOutput(512, len(masks[0]), classes, name='yolo_output_0')(x)\n",
    "\n",
    "    x = YoloConv(256, name='yolo_conv_1')((x, x_61))\n",
    "    output_1 = YoloOutput(256, len(masks[1]), classes, name='yolo_output_1')(x)\n",
    "\n",
    "    x = YoloConv(128, name='yolo_conv_2')((x, x_36))\n",
    "    output_2 = YoloOutput(128, len(masks[2]), classes, name='yolo_output_2')(x)\n",
    "\n",
    "    if training:\n",
    "        return Model(inputs, (output_0, output_1, output_2), name='yolov3')\n",
    "\n",
    "    boxes_0 = Lambda(lambda x: yolo_boxes(x, anchors[masks[0]], classes),\n",
    "                     name='yolo_boxes_0')(output_0)\n",
    "    boxes_1 = Lambda(lambda x: yolo_boxes(x, anchors[masks[1]], classes),\n",
    "                     name='yolo_boxes_1')(output_1)\n",
    "    boxes_2 = Lambda(lambda x: yolo_boxes(x, anchors[masks[2]], classes),\n",
    "                     name='yolo_boxes_2')(output_2)\n",
    "\n",
    "    outputs = Lambda(lambda x: yolo_nms(x, anchors, masks, classes),\n",
    "                     name='yolo_nms')((boxes_0[:3], boxes_1[:3], boxes_2[:3]))\n",
    "\n",
    "    return Model(inputs, outputs, name='yolov3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a39753b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:45.742987Z",
     "start_time": "2023-07-18T08:52:45.732440Z"
    }
   },
   "outputs": [],
   "source": [
    "def YoloLoss(anchors, classes=3, ignore_thresh=0.5):\n",
    "\n",
    "    def yolo_loss(y_true, y_pred):\n",
    "        # 1. transform all pred outputs\n",
    "        # y_pred: (batch_size, grid, grid, anchors, (x, y, w, h, obj, ...cls))\n",
    "        pred_box, pred_obj, pred_class, pred_xywh = yolo_boxes(\n",
    "            y_pred, anchors, classes)\n",
    "        pred_xy = pred_xywh[..., 0:2]\n",
    "        pred_wh = pred_xywh[..., 2:4]\n",
    "\n",
    "        # 2. transform all true outputs\n",
    "        # y_true: (batch_size, grid, grid, anchors, (x1, y1, x2, y2, obj, cls))\n",
    "        true_box, true_obj, true_class_idx = tf.split(y_true, (4, 1, 1),\n",
    "                                                      axis=-1)\n",
    "        true_xy = (true_box[..., 0:2] + true_box[..., 2:4]) / 2\n",
    "        true_wh = true_box[..., 2:4] - true_box[..., 0:2]\n",
    "\n",
    "        # give higher weights to small boxes\n",
    "        box_loss_scale = 2 - true_wh[..., 0] * true_wh[..., 1]\n",
    "\n",
    "        # 3. inverting the pred box equations\n",
    "        grid_size = tf.shape(y_true)[1]\n",
    "        grid = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n",
    "        grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)\n",
    "        true_xy = true_xy * tf.cast(grid_size, tf.float32) - tf.cast(grid, tf.float32)\n",
    "        true_wh = tf.math.log(true_wh / anchors)\n",
    "        true_wh = tf.where(tf.math.is_inf(true_wh), tf.zeros_like(true_wh),\n",
    "                           true_wh)\n",
    "\n",
    "        # 4. calculate all masks\n",
    "        obj_mask = tf.squeeze(true_obj, -1)\n",
    "        # ignore false positive when iou is over threshold\n",
    "        best_iou = tf.map_fn(\n",
    "            lambda x: tf.reduce_max(broadcast_iou(x[0], tf.boolean_mask(x[1], tf.cast(x[2], tf.bool))),axis=-1),\n",
    "            (pred_box, true_box, obj_mask), # input \n",
    "            tf.float32) # output type\n",
    "        ignore_mask = tf.cast(best_iou < ignore_thresh, tf.float32)\n",
    "\n",
    "        # 5. calculate all losses\n",
    "        xy_loss = obj_mask * box_loss_scale * tf.reduce_sum(tf.square(true_xy - pred_xy), axis=-1)\n",
    "        wh_loss = obj_mask * box_loss_scale * tf.reduce_sum(tf.square(true_wh - pred_wh), axis=-1)\n",
    "        obj_loss = binary_crossentropy(true_obj, pred_obj)\n",
    "        obj_loss = obj_mask * obj_loss + (1 - obj_mask) * ignore_mask * obj_loss\n",
    "        # TODO: use binary_crossentropy instead\n",
    "        class_loss = obj_mask * sparse_categorical_crossentropy(true_class_idx, pred_class)\n",
    "\n",
    "        # 6. sum over (batch, gridx, gridy, anchors) => (batch, 1)\n",
    "        xy_loss = tf.reduce_sum(xy_loss, axis=(1, 2, 3))\n",
    "        wh_loss = tf.reduce_sum(wh_loss, axis=(1, 2, 3))\n",
    "        obj_loss = tf.reduce_sum(obj_loss, axis=(1, 2, 3))\n",
    "        class_loss = tf.reduce_sum(class_loss, axis=(1, 2, 3))\n",
    "\n",
    "        return xy_loss + wh_loss + obj_loss + class_loss\n",
    "\n",
    "    return yolo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8c214c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:47.909346Z",
     "start_time": "2023-07-18T08:52:45.908873Z"
    }
   },
   "outputs": [],
   "source": [
    "model = YoloV3(size=416, training=True, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa6b1d79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:47.956563Z",
     "start_time": "2023-07-18T08:52:47.911279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"yolov3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 416, 416, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " yolo_darknet (Functional)      ((None, None, None,  40620640    ['input[0][0]']                  \n",
      "                                 256),                                                            \n",
      "                                 (None, None, None,                                               \n",
      "                                 512),                                                            \n",
      "                                 (None, None, None,                                               \n",
      "                                 1024))                                                           \n",
      "                                                                                                  \n",
      " yolo_conv_0 (Functional)       (None, 13, 13, 512)  11024384    ['yolo_darknet[0][2]']           \n",
      "                                                                                                  \n",
      " yolo_conv_1 (Functional)       (None, 26, 26, 256)  2957312     ['yolo_conv_0[0][0]',            \n",
      "                                                                  'yolo_darknet[0][1]']           \n",
      "                                                                                                  \n",
      " yolo_conv_2 (Functional)       (None, 52, 52, 128)  741376      ['yolo_conv_1[0][0]',            \n",
      "                                                                  'yolo_darknet[0][0]']           \n",
      "                                                                                                  \n",
      " yolo_output_0 (Functional)     (None, 13, 13, 3, 8  4747288     ['yolo_conv_0[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " yolo_output_1 (Functional)     (None, 26, 26, 3, 8  1194008     ['yolo_conv_1[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " yolo_output_2 (Functional)     (None, 52, 52, 3, 8  302104      ['yolo_conv_2[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 61,587,112\n",
      "Trainable params: 61,534,504\n",
      "Non-trainable params: 52,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7a61d98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:49.365381Z",
     "start_time": "2023-07-18T08:52:49.348928Z"
    }
   },
   "outputs": [],
   "source": [
    "yloss = YoloLoss(yolo_anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "128fb319",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:49.966920Z",
     "start_time": "2023-07-18T08:52:49.947501Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss = yloss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e9cab504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T08:52:53.998383Z",
     "start_time": "2023-07-18T08:52:52.309306Z"
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\rh987\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\rh987\\AppData\\Local\\Temp\\ipykernel_26008\\14234115.py\", line 6, in yolo_loss  *\n        pred_box, pred_obj, pred_class, pred_xywh = yolo_boxes(\n    File \"C:\\Users\\rh987\\AppData\\Local\\Temp\\ipykernel_26008\\2775843567.py\", line 16, in yolo_boxes  *\n        box_wh = tf.exp(box_wh) * anchors\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (yolo_loss/Exp:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 22\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# model.fit(\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     x=None,\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     y=None,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#     use_multiprocessing=False,\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7kon9dgd.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileugvvfcjs.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__yolo_loss\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     11\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     12\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 13\u001b[0m (pred_box, pred_obj, pred_class, pred_xywh) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(yolo_boxes), (ag__\u001b[38;5;241m.\u001b[39mld(y_pred), ag__\u001b[38;5;241m.\u001b[39mld(anchors), ag__\u001b[38;5;241m.\u001b[39mld(classes)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m pred_xy \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(pred_xywh)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     15\u001b[0m pred_wh \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(pred_xywh)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefkr08qzo.py:19\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__yolo_boxes\u001b[1;34m(pred, anchors, classes)\u001b[0m\n\u001b[0;32m     17\u001b[0m grid \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexpand_dims, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mstack, (ag__\u001b[38;5;241m.\u001b[39mld(grid),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), fscope)\n\u001b[0;32m     18\u001b[0m box_xy \u001b[38;5;241m=\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mld(box_xy) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(grid), ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfloat32), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(grid_size), ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfloat32), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 19\u001b[0m box_wh \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexp, (ag__\u001b[38;5;241m.\u001b[39mld(box_wh),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(anchors)\n\u001b[0;32m     20\u001b[0m box_x1y1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(box_xy) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(box_wh) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     21\u001b[0m box_x2y2 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(box_xy) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(box_wh) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"C:\\Users\\rh987\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\rh987\\AppData\\Local\\Temp\\ipykernel_26008\\14234115.py\", line 6, in yolo_loss  *\n        pred_box, pred_obj, pred_class, pred_xywh = yolo_boxes(\n    File \"C:\\Users\\rh987\\AppData\\Local\\Temp\\ipykernel_26008\\2775843567.py\", line 16, in yolo_boxes  *\n        box_wh = tf.exp(box_wh) * anchors\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (yolo_loss/Exp:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61056dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
